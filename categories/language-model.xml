<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Astropeak (Posts about language model)</title><link>https://astropeak.github.io/</link><description></description><atom:link href="https://astropeak.github.io/categories/language-model.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:astropeak@gmail.com"&gt;Astropeak&lt;/a&gt; </copyright><lastBuildDate>Fri, 04 May 2018 12:23:56 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>语言模型</title><link>https://astropeak.github.io/posts/language-model/</link><dc:creator>Astropeak</dc:creator><description>&lt;p&gt;
语言模型的定义包含两部分
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;词汇表 \(V\), 这个语言所有词的集合, 每个词表示为 \(w_i\)
&lt;/li&gt;
&lt;li&gt;概率函数 \(P(s)\), 其中 \(s\) 中所有句子组成的集合, 也即为由词汇表 \(V\) 中所有词 \(w_i\) 组成的任意长度的序列.
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
语言模型对句子的可能性进行建模。假设语言是由所有可能句子组成的集合,其中句子定义为由任意词语, 以任意顺序, 
以任意长度组成的有序序列。用 \(s\) 表示这样一个句子,则语言模型建模为一个概率分布函数 \(P(s)\), 函数值是这个句子的概率。
&lt;/p&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;例子&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
假设一个语言的词汇表只包含三个单词:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;you&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="n"&gt;ok&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
由这个词汇表所产生的所有的句子如下所示.
&lt;/p&gt;

&lt;p&gt;
长度为一的有3个:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;you&lt;/span&gt;
&lt;span class="n"&gt;are&lt;/span&gt;
&lt;span class="n"&gt;OK&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
长度为二的有9个:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;
&lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt;
&lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;span class="n"&gt;OK&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;
&lt;span class="n"&gt;OK&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt;
&lt;span class="n"&gt;OK&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
长度为三的有27个:
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt;
&lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;are&lt;/span&gt; &lt;span class="n"&gt;you&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;span class="o"&gt;...&lt;/span&gt;
&lt;span class="n"&gt;OK&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt; &lt;span class="n"&gt;OK&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;

&lt;p&gt;
长度为四的，有3×3×3×3个. 长度为五的，有3×3×3×3x3个. 这个序列是无穷的。
&lt;/p&gt;

&lt;p&gt;
然后我们有一个概率分布函数 \(P(s)\) 来计算每一个句子 \(s\) 的概率, 以下是这个函数的一些可能结果的例子:
&lt;/p&gt;
\begin{equation}
P(are\ you\ OK) = 0.008 \\
P(you\ are\ OK) = 0.002\\
P(you\ you) = 0 \\
P(OK) = 0.01 \\
P(you) = 0\\
\end{equation}
&lt;p&gt;
例子中表示句子 \("are\ you\ OK"\) 的概率为 \(0.008\) , 句子 \("you\ you"\) 的概率为 \(0\). 
&lt;/p&gt;

&lt;p&gt;
这样的一个函数 \(P(s)\) 就是这个语言的语言模型.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;语言模型的建立&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
给定一个句子 \(s = w_1w_2w_3...w_n\), 其中 \(w_i\) 是一个词. 语言模型就是要求以下函数
&lt;/p&gt;

   \begin{equation}
    P(s)=P(w_1w_2w_3...w_n) \\
  = P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_n|w_1w_2...w_{n-1})\\
= \prod_i{P(w_i|w_1w_2...w_{i-1})}
   \end{equation}

&lt;p&gt;
其中
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;\(P(w_1w_2w_3...w_n)\) 为 词序列 \(w_1w_2w_3...w_n\) 的联合概率分布
&lt;/li&gt;
&lt;li&gt;\(P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_n|w_1w_2...w_{n-1})\) 是使用的链式法则后的结果
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
这个公式可以理解为一个句子的概率是其所有单词概率的乘积, 一个单词的概率取决于他前面的所有的单词. 如果单词表的数目为5000, 
句子的长度为3，那么有1250亿种可能性, 这个在实际情况下是无法计算的.
&lt;/p&gt;

&lt;p&gt;
因此，为了使模型能够实际可计算，需要做出一个假设。语言模型中假设一个词只取决于它前面的一个词，与更之前的所有单词无关,
则上式可以转变为
&lt;/p&gt;
\begin{equation}
 P(s)=P(w_1w_2w_3...w_n) = \prod_i{P(w_i|w_{i-1})}
\end{equation}


&lt;p&gt;
这便是语言模型的简化形式: 二元语法模型(bigram model). 相应的也有三元语法模型(trigram model)，
每个词依赖于他前面的两个字。 定义如下:
&lt;/p&gt;
\begin{equation}
 P(s)=P(w_1w_2w_3...w_n) = \prod_i{P(w_i|w_{i-1}w_{i-2})}
\end{equation}

&lt;p&gt;
三元语法模型是实际中通常使用的语言模型。
&lt;/p&gt;

&lt;p&gt;
通常使用一个语料库来计算每个词的概率。语料库可以由任意文档组成。以二元模型为例，每个词的概率的计算的方法为
&lt;/p&gt;
\begin{equation}
 P(w_i|w_{i-1})= \frac{c(w_{i-1}w_i)} {\sum_w{c(w_{i-1}w)}}
\end{equation}

&lt;p&gt;
其中 
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;\(c(w_{i-1}w_i)\) 为所有以 \(w_{i-1}\) 开头，并且以 \(w_i\) 结束的二元组的数目. 
&lt;/li&gt;
&lt;li&gt;\(\sum_w{c(w_{i-1}w)}\) 表示所有以 \(w_{i-1}\) 开头, 并且以任意词结束的二元组的数目. 
&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;
概率就是这两个数目的比值.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;



&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;参数平滑方法&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
以上计算过程中有一个问题，比如计算以下这个句子的时候，句子的概率将为零。
&lt;/p&gt;


&lt;p&gt;
这里的问题是:语料库中不可能包括所有可能出现的词序列, 当某个词的组合在语料库中不存在的时候，便会导致概率为零。
此时需要使用平滑方法来解决这个问题。最简单的一种方法是加法平滑方法.
&lt;/p&gt;

&lt;p&gt;
加法平滑方法的基本原理是在统计每一个二元组的数目的时候总是为统计出的数目加个一，如下式所示
&lt;/p&gt;
\begin{equation}
 P(w_i|w_{i-1})= \frac{c(w_{i-1}w_i) + 1} {\sum_w{(c(w_{i-1}w) + 1)}} 
\end{equation}


&lt;p&gt;
这样计算后,每个词的概率的计算结果总是大于0. 
&lt;/p&gt;

&lt;p&gt;
还有很多种平滑方法，如 &lt;a href="https://en.wikipedia.org/wiki/Good%E2%80%93Turing_frequency_estimation"&gt;古德图灵方法&lt;/a&gt;，katz平滑方法等。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-4" class="outline-2"&gt;
&lt;h2 id="sec-4"&gt;语音识别中应用的例子&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
语音识别中,包含以下两个步骤
&lt;/p&gt;
&lt;ol class="org-ol"&gt;
&lt;li&gt;根据语音数据, 计算出出几种可能的句子. 因为有同音词的存在, 所以这一步可能有多个结果
&lt;/li&gt;
&lt;li&gt;根据语言模型, 计算每个句子的概率，选取概率最大的那个句子作为语音识别的结果
&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;
语言模型在第二步发挥了作用.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>language model</category><category>nlp</category><guid>https://astropeak.github.io/posts/language-model/</guid><pubDate>Thu, 02 Feb 2017 16:00:00 GMT</pubDate></item><item><title>词袋模型和N-gram模型</title><link>https://astropeak.github.io/posts/bag-of-words-and-ngram-model/</link><dc:creator>Astropeak</dc:creator><description>&lt;p&gt;
词袋模型(bag of words)是常用的语言模型，将文档看作文档中所有词的集合，而忽略词的顺序。尽管丢失了词的顺序这一属性，
但这个模型仍然能够有效用于一些自然处理任务中，如文本分类。
&lt;/p&gt;

&lt;p&gt;
N-gram模型是对词袋模型的扩展，N为一个数字，以N＝2为例，2-gram模型将文档看作文档中所有相邻两个词这些词对的集合，
也忽略这些词对在文档中出现的顺序。词袋模型是当N＝1时的特例。
&lt;/p&gt;
&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;例子&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
对于如下文档：
&lt;/p&gt;
&lt;pre class="example"&gt;
The brown fox jump up the rog.
&lt;/pre&gt;

&lt;p&gt;
对应的词袋模型为：
&lt;/p&gt;
&lt;pre class="example"&gt;
(The, brown, fox, jump, up, the, rog)
&lt;/pre&gt;

&lt;p&gt;
对应的2-gram模型为：
&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;((The, brown), (brown, fox), (fox, jump), (jump, up), (up, the), (the, rog))
&lt;/pre&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;文档表示&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
将文档表示成词袋模型后,就可以进行定量的分析。一种简单的方式是判断这个文档中是否出现词汇表中某一个词，因此可以将文档表示成一个由0和1组成的向量。
&lt;/p&gt;

&lt;p&gt;
如果语言的词汇表如下:
&lt;/p&gt;
&lt;pre class="example"&gt;
(a, boy, the, cute, likes, dog, little, girl)
&lt;/pre&gt;

&lt;p&gt;
则文档与向量表示的对应关系如下.
&lt;/p&gt;
&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="left"&gt;

&lt;col class="left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="left"&gt;文档&lt;/th&gt;
&lt;th scope="col" class="left"&gt;向量表示&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="left"&gt;a little girl&lt;/td&gt;
&lt;td class="left"&gt;[1, 0, 0, 0, 0, 0, 1, 1]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt;the body likes the dog&lt;/td&gt;
&lt;td class="left"&gt;[0, 1, 1, 0, 1, 1, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
其中向量中的每个元素表示这个文档中是否包含某个词. 如第一向量, 最后一个 1 表示它包含 girl这个词.
&lt;/p&gt;

&lt;p&gt;
进一步的可以考虑词语频率(term frequency, TF)的影响，相当于是在前一个表示的模型新增了词语频率这一信息。
词的频率此时可以作为某种度量的权重.
&lt;/p&gt;
&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="left"&gt;

&lt;col class="left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="left"&gt;文档&lt;/th&gt;
&lt;th scope="col" class="left"&gt;向量表示&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="left"&gt;a little girl&lt;/td&gt;
&lt;td class="left"&gt;[1, 0, 0, 0, 0, 0, 1, 1]&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt;the body likes the dog&lt;/td&gt;
&lt;td class="left"&gt;[0, 1, 2, 0, 1, 1, 0, 0]&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;
第二个向量的第三元素值为2,表示 the 在这个文档中出现了两次.
&lt;/p&gt;


&lt;p&gt;
有些词出现的频率非常高，可能在每一篇文档中都会出现，因此引入文档频率(doc frequency, DF)，
也即一个词在所有文档中出现的频率。 如果这个频率很高，那么这个词可能是一个停止词， 需要将其移除，
因为它对文档分类没有帮助。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;与语言模型的关系&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
词袋模型是语音模型的一种简化形式。语言模型是给定一个词汇表，然后求所有的词的序列的概率。
词袋模型则是将这个词的序列的范围大大缩小，词的序列只包含由单个词所组成的序列，则每个序列的概率则等于词频率。
&lt;/p&gt;


&lt;p&gt;
语言模型的详细介绍见&lt;a href="https://astropeak.github.io/posts/bag-of-words-and-ngram-model/language-model.html"&gt;这里&lt;/a&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id="outline-container-sec-4" class="outline-2"&gt;
&lt;h2 id="sec-4"&gt;在信息检索中的应用&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
信息检索要解决的问题是:给定一组关键字, 在所有的文档集合中, 返回与关键字相关的所有文档, 并对所有文档根据相关性进行排序.
&lt;/p&gt;

&lt;p&gt;
基本的处理方法为: 首先将所有文档表示为词袋模型,表示为一个向量,然后查询搜索关键字是否包含在这个向量里,
这样便可以计算出所有与给定关键字相关的文档. 排序文档时,可能根据词频-逆文档频率(TF-IDF)进行排序.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-5" class="outline-2"&gt;
&lt;h2 id="sec-5"&gt;在文本分类的应用&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
使用词袋模型的向量表示将每个文档表示为数学的形式,然后再输入到具体机器学习算法中.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>language model</category><category>nlp</category><guid>https://astropeak.github.io/posts/bag-of-words-and-ngram-model/</guid><pubDate>Thu, 12 Jan 2017 16:00:00 GMT</pubDate></item></channel></rss>