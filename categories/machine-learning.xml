<?xml version="1.0" encoding="utf-8"?>
<?xml-stylesheet type="text/xsl" href="../assets/xml/rss.xsl" media="all"?><rss version="2.0" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Astropeak (Posts about machine learning)</title><link>https://astropeak.github.io/</link><description></description><atom:link href="https://astropeak.github.io/categories/machine-learning.xml" rel="self" type="application/rss+xml"></atom:link><language>en</language><copyright>Contents © 2018 &lt;a href="mailto:astropeak@gmail.com"&gt;Astropeak&lt;/a&gt; </copyright><lastBuildDate>Fri, 04 May 2018 12:15:43 GMT</lastBuildDate><generator>Nikola (getnikola.com)</generator><docs>http://blogs.law.harvard.edu/tech/rss</docs><item><title>基于隐马尔科夫模型的中文分词方法</title><link>https://astropeak.github.io/posts/hmm-word-segmentation/</link><dc:creator>Astropeak</dc:creator><description>&lt;p&gt;
本文主要讲述隐马尔科夫模及其在中文分词中的应用。 基于中文分词语料库，建立中文分词的隐马尔科夫模型，最后用维特比方法进行求解。
&lt;/p&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;一、隐马尔科夫模型介绍&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
隐马尔科夫模型中包括两个序列，其中一个是观测序列，另一个是隐藏序列。模型要解决的一个问题是，给定观测序列，
求其对应的隐藏序列。比如对于语音识别，这里的观测序列是语音的每一个序列，隐藏序列是这一串语音对应的文字。
或者对于机器翻译，观测序列是待翻译的语言，而隐藏序列则是目标语言。在解决这类问题时，我们的已知条件是，
第一，隐藏序列中某一个元素到观测序列中某一个元素之间的映射关系。第二是隐藏序列中每个元素转变到另一个元素之间的关系。
并且会有两个假设，第一是每个隐藏元素中的元素，只依赖于它前面一个元素。第二是每一个隐藏元素能够直接确定另一个观测元素。
其中以上两个已知条件可以分别表示为两个矩阵，这个矩阵可以通过分词语料库，根据统计的方法求得。
&lt;/p&gt;


&lt;p&gt;
从数学上理解，给定观测序列求解隐藏序列。一个观测序列可能对应无数个隐藏序列，而我们的目标隐藏序列就是，
概率最大的那一个隐藏序列，也就是说最有可能的那个序列。即求给定隐藏序列这个条件下观测序列概率最大的那个序列的条件概率问题。
&lt;/p&gt;

&lt;p&gt;
虽然隐马尔科夫模型中做了比较强的假设，这里比较强的意思是，现实生活中，可能根本无法满足这种假设条件，
但是它的应用范围却是比较广泛，能够非常简单有效的解决复杂的问题。这个也是统计学方法的一个特点, 
基于大量的数据，使用简单的方法便可以解决复杂的问题。这可能是大数据的一种威力吧，如果我们有足够多的数据，
那么我们的模型可以做简化。而如果我们只有少量的数据，则我们必须用非常精确的模型，才可以得到我们想要的结果。
数据模型以及数据量也是一种此消彼长的关系。
&lt;/p&gt;

&lt;p&gt;
下图为隐马尔科夫模型的概率图，其中 \(x\) 代表隐藏序列， \(y\) 代表观察序列。
&lt;img src="https://astropeak.github.io/posts/hmm-word-segmentation/img/hmm.png" alt="hmm.png"&gt;
&lt;/p&gt;

&lt;p&gt;
其中这里的隐藏序列，又称为一个马尔科夫链。马尔科夫链的定义是，序列中每一个元素，只依赖于他前面的一个元素而与其他所有元素不相关。
&lt;/p&gt;


&lt;p&gt;
为了使用隐马尔科夫模型，我们必须知道两个矩阵。第一个矩阵式表示隐藏序列中, 一个元素转变到另一个元素的概率.
如果总的序列的元素数目为n，则这个是一个n乘n的矩阵。 
第二个矩阵表示隐藏序列中某个元素转变到观测序列中某个元素的概率. 如果隐藏序列中元素个数为n，观测序列中元素的数目为m，
则这个矩阵的维数为n乘m。 这两个矩阵都可以通过大量的统计数据来求得。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;第二、中文分词的隐马尔科夫模型&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
中文分词要解决的问题是，给定一段中文文字，将其划分为一个个单独的词或者单字。中文分词是所有后续自然语言处理的基础。
如果将连续的中文文字看作是观测序列，将每个文字所对应的分词状态看作是隐藏序列，每个字的分子状态可能有两个值，
一个表示这个字是某一个词的词尾，用字母E表示。另一个表示这个字不是某一个词的词尾，用字母B表示。
则中文分词问题可以看作是一个标准的隐马尔科夫模型。实际中将每个字的分子状态表示为四个可选的值。
词的开始，词的中间，词的结尾，单字成词。分别用bmes表示。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;第三、中文分词语料库&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
在网上可以免费下载，北京大学和香港大学提供的中文分词语料库。这个语料库实际上是一个txt的文档，
文档中每个单独的词用两个空格隔开。
根据分词语料库，我们可以求得隐马尔科夫模型中的两个参数矩阵. 根据大数定理，概率等于次数的比值。因此为了模型的准确性，
我们必须有大量的语料数据来计算模型的参数。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-4" class="outline-2"&gt;
&lt;h2 id="sec-4"&gt;第四、使用维特比算法进行求解&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
给定马尔科夫模型中的两个参数以及一个观测序列，求解隐藏序列的问题，可以看作是一个类似于图论中的最短路径问题.
&lt;/p&gt;

&lt;p&gt;
维特比算法使用动态规划的方法进行求解。动态规划的子问题是，对于第 \(i\) 列的每一个元素 \(A_j\), 
以这个元素为最后一个节点的最短路径。如果知道第 \(i\) 列的每一个元素的最短路径，那么第 \(i+1\) 列的每个元素的最短路径变可求得.
&lt;/p&gt;

&lt;p&gt;
最终的结果就是第 \(n\) 列中每一个元素的最短路径中值最小的那一个元素所对应的最短路径。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-5" class="outline-2"&gt;
&lt;h2 id="sec-5"&gt;第五、代码实现&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
&lt;a href="https://github.com/astropeak/nlp-word-segmentation"&gt;nlp-word-segmentation&lt;/a&gt;
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>hmm</category><category>machine learning</category><category>nlp</category><guid>https://astropeak.github.io/posts/hmm-word-segmentation/</guid><pubDate>Fri, 05 May 2017 16:00:00 GMT</pubDate></item><item><title>生成模型和判定模型</title><link>https://astropeak.github.io/posts/generative-model-and-descriminative-model/</link><dc:creator>Astropeak</dc:creator><description>&lt;p&gt;
统计概率模型的作用是给定输入 \(x\) ， 预测输出 \(y\) 的概率，也即求解条件概率 \(P(y|x)\) 。统计概率模型分为两种：生成模型（Generative Model）和判定模型（Descriminative Model）。
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;如果模型学习的是 \(P(x, y)\) 的联合概率分布，则这个模型是生成模型。求得 \(P(x, y)\) 后，根据贝叶斯定律，可间接求得 
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;
\(P(y|x) = \frac{P(x, y)}{P(x)}\) 。
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;如果模型学习的是 \(P(y|x)\) 的条件概率分布，则这个模型是判定模型。此时是直接求解。
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
常见的生成模型和判定模型如下
&lt;/p&gt;
&lt;table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides"&gt;


&lt;colgroup&gt;
&lt;col class="left"&gt;

&lt;col class="left"&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th scope="col" class="left"&gt;生成模型&lt;/th&gt;
&lt;th scope="col" class="left"&gt;判定模型&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class="left"&gt;朴素贝叶斯&lt;/td&gt;
&lt;td class="left"&gt;罗辑斯谛回归&lt;/td&gt;
&lt;/tr&gt;

&lt;tr&gt;
&lt;td class="left"&gt;隐马尔科夫模型&lt;/td&gt;
&lt;td class="left"&gt;条件随机场&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;为什么叫生成模型&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
  生成模型的中的生成表示: 根据训练数据, 模型学习给定 \(y\) 生成 \(x\) 的概率，即学习 \(P(x|y)\) 。以朴素贝叶斯为例，
求解公式为
&lt;/p&gt;
  \begin{equation}
y^* = \underset{y}{\operatorname{argmax}} P(y|x) = \underset{y}{\operatorname{argmax}} P(x|y)P(y)
\end{equation}

&lt;p&gt;
\(P(x|y)\) 表示给定分类 \(y\) ，来预测可能的 \(x\) 值，或者生成 \(x\) 的值。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;区别&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
判定模型由于是直接学习 \(P(y|x)\) ，因此在相同数目的训练数据条件下，通常其预测效果要好于生成模型。
&lt;/p&gt;

&lt;p&gt;
生成模型的功能更加强大。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>machine learning</category><guid>https://astropeak.github.io/posts/generative-model-and-descriminative-model/</guid><pubDate>Sat, 22 Apr 2017 16:00:00 GMT</pubDate></item><item><title>梯度下降算法</title><link>https://astropeak.github.io/posts/gradient-descend/</link><dc:creator>Astropeak</dc:creator><description>&lt;p&gt;
梯度下降算法是一种用于求解函数最小值的数值计算方法。求解函数的极值一般有两种方法，第一种是解析法, 
即函数的极值可以用公式直接计算出来。如对于1元2次方程，我们可以直接求出这个函数的最大值或者最小值。
但对于更高次的方程或者非线性函数, 大多无法求出解析解，此时需要使用数值方法.
&lt;/p&gt;


&lt;p&gt;
对于一元函数来说, 函数的梯度就是其导数. 假设函数为
&lt;/p&gt;


\begin{equation}
f(x) = 2x^2-4x+3
\end{equation}

&lt;p&gt;
则函数的梯度函数，也即导数为
&lt;/p&gt;

\begin{equation}
Grad(x) = 4x-4
\end{equation}

&lt;p&gt;
梯度下降算法的基本原理如下:
&lt;/p&gt;

&lt;p&gt;
对于任意一个自变量 \(x\) 的值 \(x_1\), 我们可以计算此时函数的梯度为 \(Grad(x_1)\), 用当前的 \(x\) 的值 \(x_1\)
减去梯度值的一个比例, 得到一个新的自变量 \(x\) 的值 \(x_2 = x_1 - 0.01*Grad(x_1)\). 则函数在这个新的自变量 \(x\)
的值的地方肯定是小于原来的值的地方, 即
&lt;/p&gt;
\begin{equation}
f(x_2) &amp;lt; f(x_1)
\end{equation}
&lt;p&gt;
其中 \(0.01\) 为步长值.
&lt;/p&gt;


&lt;p&gt;
比如我们取 \(x_1 = 2\), 此时梯度值为 \(4*2-4=4\), 则 \(x_2 = 2 - 0.01*(4) = 1.96\),
&lt;/p&gt;

\begin{equation}
f(x_1) = 2*2^2-4*2+3 = 3\\
f(x_2) = 2*1.96^2-4*1.96+3 = 2.843
\end{equation}

&lt;p&gt;
可见 \(f(x_2) &amp;lt; f(x_1)\)
&lt;/p&gt;

&lt;p&gt;
其中
&lt;/p&gt;
&lt;ul class="org-ul"&gt;
&lt;li&gt;\(x_1=2\) 中的 \(2\) 为自变量的初值. 初值可以随机选择, 如果函数是凸的, 不管初值为何,总能找到相同的极值点
&lt;/li&gt;
&lt;li&gt;\(0.01\) 为每次迭代的步长值
&lt;/li&gt;
&lt;/ul&gt;


&lt;p&gt;
重复以上步骤, 选定初值后,根据以上方法不断变化自变量的值, 随着我们每一次的迭代，自变量的值将慢慢向最优值所对应的 \(x\) 值逼近
(对于这个例子, 最优值对应的 \(x = 1\)). 那么只要迭代的次数足够多，我们总会达到这个最优点，由此便求得函数的最小值.
&lt;/p&gt;

&lt;div id="outline-container-sec-1" class="outline-2"&gt;
&lt;h2 id="sec-1"&gt;梯度的概念&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-1"&gt;
&lt;p&gt;
以上提到对于一元函数, 其梯度就是函数的导数,梯度值为一个数值。梯度概念是对导数的扩展, 当函数有多于一个自变量的时候.
只是梯度此时是一个向量,向量的每一维分别是对于每一个自变量求偏导数。
&lt;/p&gt;

&lt;p&gt;
以一个二元函数为例
&lt;/p&gt;
\begin{equation}
f(x_1, x_2) = (x_1-2)^2 + 3x_2
\end{equation}

&lt;p&gt;
函数的梯度函数为
&lt;/p&gt;
\begin{equation}
Grad(x_1) = \frac{\partial f}{\partial x_1} = 2x_1-4\\
Grad(x_2) = \frac{\partial f}{\partial x_2}=3\\
Grad(x_1, x_2) = [Grad(x_1), Grad(x_2)] = [2x_1-4, 3]
\end{equation}


&lt;p&gt;
以上讲到的关于自变量沿梯度反方向变化时, 函数值将减小的规则同样适用于二元及更高元的函数. 梯度下降算法的原理也一样,
不同点为此时更新自变量时,是向量操作.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-2" class="outline-2"&gt;
&lt;h2 id="sec-2"&gt;函数的等值线&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-2"&gt;
&lt;p&gt;
梯度总是垂直于函数的等值线。从等值线中可以更加形象地看出梯度下降的原理. 当自变量沿着梯度的方向变化时, 
函数值将增加, 等值线所对应的值会增加. 因此，当我们将自变量沿着梯度的反方向变化时，函数的值将会减小. 
由此我们可以求出函数的一个极小值, 通过让自变量不断的向梯度的反方向变化.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;

&lt;div id="outline-container-sec-3" class="outline-2"&gt;
&lt;h2 id="sec-3"&gt;步长值&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-3"&gt;
&lt;p&gt;
  梯度给出了自变量变化的方向, 而步长值指定了每次变化的幅度. 如果步长值选得过大，则可能会导致在一次更新自变量后,
错过极值点，从而造成震荡, 无法求得函数的极值. 因此我们要保证步长值足够小，避免震荡现象的出现.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-4" class="outline-2"&gt;
&lt;h2 id="sec-4"&gt;收敛准则&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-4"&gt;
&lt;p&gt;
由于是通过迭代的方式一步一步的改变自变量的值来求函数的极值, 那么怎样判断函数已经达到一个极值了呢? 
此时需要有一个收敛准则, 即判断一次迭代后,函数是否达到极值点. 一般使用的收敛准则为:指定一个精确率, 
当函数值的变化率小于这个精确率时，我们就认为收敛了，此时函数的极值已经被找到.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;


&lt;div id="outline-container-sec-5" class="outline-2"&gt;
&lt;h2 id="sec-5"&gt;在机器学习算法中的应用&lt;/h2&gt;
&lt;div class="outline-text-2" id="text-5"&gt;
&lt;p&gt;
对于一个机器学习算法，如果其目标函数可以表示成连续可微的凸函数，则我们可以用梯度下降算法进行求解其最小值或者最大值.
&lt;/p&gt;

&lt;p&gt;
例如在对数几率回归算法中, 可以将目标函数写成一个连续可微的凸函数, 然后用梯度下降算法求函数的最小值, 
从而求得模型的参数。
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;</description><category>gradient descend</category><category>machine learning</category><guid>https://astropeak.github.io/posts/gradient-descend/</guid><pubDate>Tue, 31 Jan 2017 16:00:00 GMT</pubDate></item></channel></rss>