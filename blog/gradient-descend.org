#+TITLE:       梯度下降算法
#+AUTHOR:      Astropeak
#+EMAIL:       astropeak@gmail.com
#+DATE:        2017-02-01
#+URI:         /blog/%y/%m/%d/gradient-descend
#+KEYWORDS:    machine learning, gradient descend
#+TAGS:        machine learning, gradient descend
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: gradient descend algorithm


梯度下降算法是一种用于求解函数最小值的数值计算方法。求解函数的极值一般有两种方法，第一种是解析法记函数的极值
可以用公式直接计算出来。如对于1元2次方程，我们可以直接求出这个函数的最大值或者最小值。
但对于更高次的方程或者非线性函数, 大多无法求出解析解，此时需要使用数值方法.

对于一元函数来说, 函数的梯度就是其导数. 假设函数为

$f = 2x^2-4x+3$

则函数的梯度函数，也即导数为

$grad = 4x-4$

梯度下降算法的基本原理如下:

对于任意一个自变量 $x$ 的值 $x_1$, 我们可以计算此时函数的梯度为 $grad(x_1)$, 用当前的 $x$ 的值 $x_1$ 
减去梯度值的一个比例, 得到一个新的自变量 $x$ 的值 $x_2 = x_1 - 0.01*grad(x_1)$. 则函数在这个新的自变量 $x$ 
的值的地方肯定是小于原来的值的地方, 即

$f(x_2) < f(x_1)$


比如我们取 $x_1 = 2$, 此时梯度值为 $4*2-4=4$, 则 $x_2 = 2 - 0.01*(4) = 1.96$, 

$f(x_1) = 2*2^2-4*2+3 = 3$

$f(x_2) = 2*1.96^2-4*1.96+3 = 2.843$


可见 $f(x_2) < f(x_1)$ 


由上面的步骤可以看出，随着我们每一次的迭代，自变量的值慢慢向最优值所对应的 $x$ 值逼近. 那么只要迭代的次数足够多，
我们总会达到这个最优点，由此求得函数的最小值.


* 梯度的概念
以上提到对于一元函数, 其梯度就是函数的导数,梯度值为一个数值。梯度概念是对导数的扩展, 当函数有多于一个自变量的时候.
只是梯度此时是一个向量,向量的每一维分别是对于每一个自变量求偏导数。

以一个二元函数为例，函数的梯度函数为。



* 函数的等值线
力度总是垂直于函数的等值线。从等值线中可以更加形象地看出梯度下降的原理. 当自变量沿着梯度的方向变化时, 数值将增加, 以及等值线所对应的值会增加. 因此，当我们将便自变量沿着梯度的反方向变化时，函数的值将会减小. 由此，我们可以求出函数的一个极小值, 我让自变量不断的向梯度的反方向变化.


* 收敛准则
由于我们是通过迭代的方式一步一步的改变自变量的值来求函数的极值 那么我们怎样判断函数已经达到一个极致了呢 我们需要有一个收敛准则. 一般使用的时候两种测试指定一个精确率, 当函数值的变化率小于这个精确历史，我们就认为收敛了，此时函数的机制已经被找到


* 步长值
不常只指定了我们每次变化梯度变化自变量的幅度. 梯度，只是给出了我们变化的方向, 而补偿职责制定了变化的幅度. 如果不长这选得过大，则可能会导致错过机智点，从而造成震荡,数值不收敛. 因此我们要保证不缠着足够小，避免震荡现象的出现. 




* 在机器学习算法中的应用
对于一个机器学习算法，如果其目标函数可以表示成连续可微的凸函数，则我们可以用梯度下降算法进行求解，其最小值或者最大值.例如
在对数几率回归办法中。, 我们可以将目标函数写成一个连续可微的凸函数。, 然后用梯度算法梯度下降算法求函数的最小只有四球的模型的参数。

