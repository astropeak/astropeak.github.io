#+TITLE:       语言模型
#+AUTHOR:      Astropeak
#+EMAIL:       astropeak@gmail.com
#+DATE:        2017-02-03
#+URI:         /blog/%y/%m/%d/language-model
#+KEYWORDS:    nlp, language model
#+TAGS:        nlp, language model
#+LANGUAGE:    en
#+OPTIONS:     H:3 num:nil toc:nil \n:nil ::t |:t ^:nil -:nil f:t *:t <:t
#+DESCRIPTION: language model
#+HTML_MATHJAX: align: left mathml: t indent: 5em tagside: left font: Neo-Euler

语言模型的定义包含两部分
1. 词汇表 $V$, 这个语言所有词的集合, 每个词表示为 $w_i$
2. 概率函数 $P(s)$, 其中 $s$ 中所有句子组成的集合, 也即为由词汇表 $V$ 中所有词 $w_i$ 组成的任意长度的序列.

语言模型对句子的可能性进行建模。假设语言是由所有可能句子组成的集合,其中句子定义为由任意词语, 以任意顺序, 
以任意长度组成的有序序列。用 $s$ 表示这样一个句子,则语言模型建模为一个概率分布函数 $P(s)$, 函数值是这个句子的概率。

* 例子
假设一个语言的词汇表只包含三个单词:
#+begin_src python
  [you, are, ok]
#+end_src

由这个词汇表所产生的所有的句子如下所示.

长度为一的有3个:
#+begin_src python
  you
  are
  OK
#+end_src

长度为二的有9个:
#+begin_src python
  you you
  you are
  you OK
  are you
  are are
  are OK
  OK you
  OK are
  OK OK
#+end_src

长度为三的有27个:
#+begin_src python
  you you you
  you you are
  you you OK
  you are you
  you are are
  you are OK
  ...
  are you OK
  ...
  OK OK OK
#+end_src

长度为四的，有3×3×3×3个. 这个序列是无穷的。

然后我们有一个概率分布函数 $P(s)$ 来计算每一个句子 $s$ 的概率, 以下是这个函数的一些可能结果的例子:
    \begin{equation}
    P(are\ you\ OK) = 0.008 \\
    P(you\ are\ OK) = 0.002\\
    P(you\ you) = 0 \\
    P(OK) = 0.01 \\
    P(you) = 0\\
    \end{equation}
例子中表示句子 $"are you OK"$ 的概率为0.008, 句子 $"you you"$ 的概率为0. 

这样的一个函数 $P(s)$ 就是这个语言的语言模型.

* 语言模型的建立
给定一个句子 $s = w_1w_2w_3...w_n$, 其中 $w_i$ 是一个词. 其联合分布概率为表示为

    \begin{equation}
     P(s)=P(w_1w_2w_3...w_n) \\
   = P(w_1)P(w_2|w_1)P(w_3|w_1w_2)...P(w_n|w_1w_2...w_{n-1})\\
 = \prod_i{P(w_i|w_1w_2...w_{i-1})}
    \end{equation}

这个公式可以理解为一个句子的概率是其所有单词概率的乘积, 一个单词的概率取决于他前面的所有的单词. 如果单词表的数目为5000, 
句子的长度为3，那么有1250亿种可能性, 这个在实际情况下是无法计算的.

因此，为了使模型能够实际可计算，需要做出一个假设。语言模型中假设一个词只取决于它前面的一个词，与更之前的所有单词无关,
则式1可以转变为
    \begin{equation}
     P(s)=P(w_1w_2w_3...w_n) = \prod_i{P(w_i|w_{i-1})}
    \end{equation}


这便是语言模型的简化形式: 二元语法模型(bigram model). 相应的也有三元语法模型(trigram model)，
每个词依赖于他前面的两个字。 定义如下:
    \begin{equation}
     P(s)=P(w_1w_2w_3...w_n) = \prod_i{P(w_i|w_{i-1}w_{i-2})}
    \end{equation}

三元语法模型是实际中通常使用的语言模型。

通常使用一个语料库来计算每个词的概率。语料库可以由任意文档组成。以二元模型为例，每个词的概率的计算的方法为
    \begin{equation}
     P(w_i|w_{i-1})= \frac{c(w_{i-1}w_i)} {\sum_w{c(w_{i-1}w)}}
    \end{equation}

其中 $c(w_{i-1}w_i)$ 为所有以 $w_{i-1}$ 开头，并且以 $w_i$ 结束的二元组的数目. $\sum_w{c(w_{i-1}w)}$ 
表示所有以 $w_{i-1}$ 开头, 并且以任意词结束的二元组的数目. 概率就是这两个数目的比值.

* 一个实际计算的例子
假设我们的语料库由以下三个句子组成



对于这个句子体感的计算过程如下



* 参数平滑方法
以上计算过程中有一个问题，比如计算一下这个句子的时候，句子的概率将为零。

这里的问世问题是 语料库中不可能包括所有的吃的 当某个词的组合在语料库中不存在的时候，便会导致嗯概率为零。 通过使用平仓出的平滑方法来解决这个问题。 最简单的一种方法是加法平滑方法
贾平华方法的基本原理是在统计每一个二元组的数目的时候总是为其的数目加个一，是变为以下，这个式子


根据这个方法，我们计算上面这个句子，它的概率为

结果比干率为零要合理的合理。

还有很多种品牌方法，如古得图灵方法，katz平滑方法等。可只给出链接。


* 语音识别中应用的例子
根据语音数据会给出几种可能的句子，因为有同音词的存在。然后通过计算，根据语言模型计算每个句子的概率，选取概率最大的那个句子便是语音识别的结果。










